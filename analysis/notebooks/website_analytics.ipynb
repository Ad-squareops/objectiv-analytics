{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import sqlalchemy\n",
    "\n",
    "# import Objectiv buh_tuh\n",
    "from buhtuh import BuhTuhDataFrame\n",
    "sys.path.extend([\n",
    "    '../../buhtuh',\n",
    "    '../'\n",
    "])\n",
    "\n",
    "from objectiv_buhtuh.util import duplo_basic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b47e4-e1c6-443f-a164-31078ed3ae8b",
   "metadata": {},
   "source": [
    "## Get website production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to full postgresql dataset, add database and credentials here\n",
    "engine = sqlalchemy.create_engine('postgresql://objectiv:@localhost:5432/objectiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buh_tuh dataframe based on the full dataset\n",
    "basic_features = duplo_basic_features()\n",
    "full_df = BuhTuhDataFrame.from_model(engine=engine, model=basic_features, index=['event_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a13bf-52e0-4082-a4a2-c382871b1713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set the timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177f1fb-c741-4269-8086-dbab942afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timeframe for analysis\n",
    "timeframe_selector = (full_df['moment'] >= datetime.date(2021,6,1)) & (full_df['moment'] < datetime.date(2021,11,1))\n",
    "\n",
    "# create one sampled df with timeframe applied \n",
    "timeframe_df = full_df[timeframe_selector]\n",
    "\n",
    "# explore the data\n",
    "timeframe_df.sort_values(by='moment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4954-cabc-44e7-81b8-d18f9d303ed9",
   "metadata": {},
   "source": [
    "## Set the time aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a697920-a443-4593-8acd-71c514aa7406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose for which level of time aggregation the rest of the analysis will run\n",
    "# supports all Postgres datetime template patterns: https://www.postgresql.org/docs/9.1/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE\n",
    "\n",
    "agg_level = 'YYYYIW'\n",
    "\n",
    "# add the time aggregation as new column to the dataframes, so we can group on this later\n",
    "timeframe_df['time_aggregation'] = timeframe_df['moment'].format(agg_level)\n",
    "full_df['time_aggregation'] = full_df['moment'].format(agg_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cd456-699f-4e92-b225-3b565504683d",
   "metadata": {},
   "source": [
    "## Set the user application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba425d-ccfd-4c4e-bf18-3a0195d605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to dataframes with the user application from the global contexts\n",
    "timeframe_df['user_application'] = timeframe_df.global_contexts.json.application\n",
    "full_df['user_application'] = full_df.global_contexts.json.application\n",
    "\n",
    "# select one or more user application(s) for analysis, in this case objectiv.io website \n",
    "# when selecting more than one application, each of the metrics below can easily be group by user_application to compare behavior\n",
    "timeframe_df = timeframe_df[(timeframe_df['user_application'] == 'objectiv-website')]\n",
    "full_df = full_df[(full_df['user_application'] == 'objectiv-website')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697e0d3-8443-427b-a3a9-09342adf624e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00ffd-9940-4ff3-b5db-2734730f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique users per timeframe\n",
    "users = timeframe_df.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# calculate total users, to reuse later\n",
    "total_users = timeframe_df['user_id'].nunique()\n",
    "\n",
    "users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d9b1e-5159-416d-a904-75b63b795e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize users\n",
    "users.sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f5454-f7c7-409b-a6d0-996e078c851a",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3c2a3-9616-43d6-b529-acf5f6d210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique sessions\n",
    "sessions = timeframe_df.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "sessions.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc334-17c9-4604-85dd-24fa43abaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sessions\n",
    "sessions.sort_values('time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888435d-4287-4cdc-8767-985fe2b9c216",
   "metadata": {},
   "source": [
    "## Sessions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77706040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge users and sessions\n",
    "users_sessions = sessions.merge(users, how='inner', on='time_aggregation')\n",
    "\n",
    "# calculate average sessions per user\n",
    "users_sessions['sessions_per_user_avg'] = users_sessions['session_id_nunique'] / users_sessions['user_id_nunique']\n",
    "\n",
    "# clean-up columns\n",
    "users_sessions.drop(columns=['session_id_nunique', 'user_id_nunique'], inplace=True)\n",
    "\n",
    "users_sessions.sort_values('time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d52d8-08df-4133-a2da-6cedd81f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize average sessions per user\n",
    "users_sessions.sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaaf82-ee41-4602-a2cd-f622fa84b0b2",
   "metadata": {},
   "source": [
    "## New users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917e1a3-5b16-4365-98e0-ea0040babebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first seen per user, based on full dataset\n",
    "user_first_seen = full_df.groupby('user_id').aggregate({'time_aggregation':'min', 'session_id':'min'})\n",
    "\n",
    "# calculate new users for each timeframe\n",
    "new_users = user_first_seen.groupby('time_aggregation_min').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with total users, to calculate ratio and limit to timerange\n",
    "new_total_users = users.merge(new_users, how='inner', left_on='time_aggregation', right_on='time_aggregation_min', suffixes=('_total', '_new'))\n",
    "\n",
    "# set time_aggregation as single index\n",
    "new_total_users = new_total_users.set_index('time_aggregation')\n",
    "\n",
    "# calculate new & returning user share\n",
    "new_total_users['new_user_share'] = new_total_users['user_id_nunique_new'] / new_total_users['user_id_nunique_total']\n",
    "new_total_users['returning_user_share'] = (new_total_users['user_id_nunique_total'] - new_total_users['user_id_nunique_new']) / new_total_users['user_id_nunique_total']\n",
    "\n",
    "new_total_users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d062-dd61-49c2-b669-bf0f45c68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize new users\n",
    "new_total_users[['user_id_nunique_new', 'user_id_nunique_total']].sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be740b11-d552-4501-b8b3-48cb077f8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize returning users\n",
    "new_total_users[['returning_user_share']].sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046eb7e6-fba5-4048-ba17-8cff59d92b38",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c743b4-dae4-41b9-a4d5-15a87c3b295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the event location from the location_stack as new column to the df\n",
    "timeframe_df['event_location'] = timeframe_df.location_stack.json.nice_name\n",
    "\n",
    "# get the number of total users and hits per feature\n",
    "users_per_event = timeframe_df.groupby(['time_aggregation', 'event_type', 'event_location']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n",
    "\n",
    "users_per_event.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8695f-fbc9-4996-8b06-cea971d7baac",
   "metadata": {},
   "source": [
    "## New user events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dca3b5-4a48-454b-a519-bcb341b8a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 10 things new users do\n",
    "\n",
    "# get the first session for users that were new in the timeframe\n",
    "timeframe_new_users = timeframe_df.merge(user_first_seen, how='inner', left_on=['user_id', 'time_aggregation', 'session_id'], right_on=['user_id', 'time_aggregation_min', 'session_id_min'])\n",
    "\n",
    "# limit to the first 10 events\n",
    "timeframe_new_users = timeframe_new_users[(timeframe_new_users['session_hit_number'] <= 10)]\n",
    "\n",
    "# number of total user and hits per feature\n",
    "new_user_events = timeframe_new_users.groupby(['time_aggregation', 'event_type', 'event_location']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n",
    "\n",
    "new_user_events.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf9cf5-c4e6-4d71-a15d-64bea1bf4d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## WIP Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e09155-217c-4631-b528-ccb7be25ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - find conversion feature - replace with sankey when we have it \n",
    "\n",
    "# set the completed conversion event, in the case a successful submission of email address to keep-me-posted\n",
    "# NOTE: replace soon with the completed event '(WebDocumentContext,#document),(SectionContext,keep-me-posted-form),(CompletedContext,keep-me-posted)'\n",
    "conversion_completed = '(WebDocumentContext,#document),(SectionContext,keep-me-posted-form),(ActionContext,keep-me-posted)'\n",
    "\n",
    "# filter on only completed conversion events\n",
    "conversion_completed = timeframe_df[(timeframe_df['feature'] == conversion_completed)]\n",
    "\n",
    "# calculate conversions, now per user, but can easily be aggregated to session_id instead\n",
    "conversions = conversion_completed.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with users, but can easily be done with sessions instead\n",
    "conversion_rate = conversions.merge(users, how='inner', on='time_aggregation', suffixes=('_converting', '_total'))\n",
    "\n",
    "# calculate conversion rate\n",
    "conversion_rate['conversion_rate'] = conversion_rate['user_id_nunique_converting'] / conversion_rate['user_id_nunique_total']\n",
    "\n",
    "conversion_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77c8ee-6038-4b9a-9403-09502899511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize conversion rate\n",
    "conversion_rate[['conversion_rate']].sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a49676-1373-412b-b1bf-22974ad44d5b",
   "metadata": {},
   "source": [
    "## WIP Conversion error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd50963-7aba-4fa2-8329-15e56909a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE replace with sankey when we have it\n",
    "\n",
    "# set the conversion start event, which can turn out completed or an error, on this case a click on 'keep me posted' button\n",
    "conversion_start = '(WebDocumentContext,#document),(SectionContext,header),(SectionContext,keep-me-posted-form),(ButtonContext,subscribe)'\n",
    "\n",
    "# filter on only completed conversion start events\n",
    "conversion_start = timeframe_df[(timeframe_df.feature == conversion_start)]\n",
    "\n",
    "# calculate conversion starts, now per user, but can easily be aggregated to session_id instead\n",
    "conversion_starts = conversion_start.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# join conversion start & complete events\n",
    "conversion_totals = conversion_starts.merge(conversions, how='left', on='time_aggregation')\n",
    "\n",
    "# rename columns\n",
    "conversion_totals.rename(columns={'user_id_nunique_x':'users_start','user_id_nunique_y':'users_completed'}, inplace=True)\n",
    "\n",
    "# calculate error rate by comparing starting and successfully completing a conversion event\n",
    "conversion_totals['error_rate'] = (conversion_totals['users_start'] - conversion_totals['users_completed']) / conversion_totals['users_start']\n",
    "\n",
    "conversion_totals.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84339df-e25a-43ff-a527-8ff7aa674128",
   "metadata": {},
   "source": [
    "## WIP Conversion funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ea3f2-a2b0-4701-80fe-3cce1e54b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for users that have a conversion event, select the first one (later we can add multiple conversions case)\n",
    "converting_users = conversion_completed.groupby(['user_id']).aggregate({'moment':'min'})\n",
    "\n",
    "# merge with the df that has all user events in the timeframe\n",
    "converting_users_events = timeframe_df.merge(converting_users, how='inner', on='user_id')\n",
    "\n",
    "# select all events that converting users had up to their first conversion moment\n",
    "converting_users_events = converting_users_events[(converting_users_events['moment'] <= converting_users_events['moment_min'])]\n",
    "\n",
    "# create a window that returns the previous event for each row\n",
    "window = converting_users_events.sort_values('moment').window('session_id')\n",
    "converting_users_events['prev_event'] = converting_users_events.feature.window_lag(window)\n",
    "converting_users_events['prev_moment'] = converting_users_events.moment.window_lag(window)\n",
    "\n",
    "converting_users_events[['user_id', 'moment', 'event_type' , 'event_location', 'prev_moment', 'prev_event']].sort_values(by=['user_id', 'moment'], ascending=True).head()\n",
    "\n",
    "# TODO use this as input for a sankey visual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c06f91-3fe1-4e25-8706-601e1b223393",
   "metadata": {},
   "source": [
    "## Session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279e793-e5a4-4a4b-968a-db8c5949f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate duration of each session\n",
    "session_duration = timeframe_df.groupby(['session_id']).aggregate({'moment':['min','max'], 'time_aggregation':'min'})\n",
    "session_duration['session_duration'] = session_duration['moment_max'] - session_duration['moment_min']\n",
    "\n",
    "# check which sessions have duration of zero and filter these out, as they are bounces\n",
    "session_duration = session_duration[(session_duration['session_duration'] > '0')]\n",
    "\n",
    "# rename columns\n",
    "session_duration.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate average session duration\n",
    "avg_session_duration = session_duration.groupby(['time_aggregation']).aggregate({'session_duration': 'mean'})\n",
    "\n",
    "avg_session_duration.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0f2c5-8e20-4b54-bdfa-dcb920794e5d",
   "metadata": {},
   "source": [
    "## WIP Session duration between events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb0024-91db-4a8b-a045-21934c117020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: replace event selection with sankey\n",
    "\n",
    "# define the start and stop events to measure the duration in between, in this case landing on homepage and completing conversion\n",
    "start_event = '(WebDocumentContext,#document)'\n",
    "stop_event = '(WebDocumentContext,#document),(SectionContext,keep-me-posted-form),(ActionContext,keep-me-posted)'\n",
    "\n",
    "# filter on only these events\n",
    "start_stop = timeframe_df[(timeframe_df.feature == start_event) | (timeframe_df.feature == stop_event)]\n",
    "\n",
    "# get previous (because of the sorting) event for stop event _in the same session, window_lag(n) returns the nth previous value in the partition\n",
    "window = start_stop.sort_values('moment').window('session_id')\n",
    "start_stop['prev_event'] = start_stop.feature.window_lag(window)\n",
    "start_stop['prev_moment'] = start_stop.moment.window_lag(window)\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "start_stop = start_stop.get_df_materialized_model()\n",
    "\n",
    "# filter: for each stop event, select the closest preceeding start event\n",
    "complete = start_stop[(start_stop.feature == stop_event) & (start_stop.prev_event == start_event)]\n",
    "\n",
    "# calculate duration\n",
    "complete['duration'] = complete.moment - complete.prev_moment\n",
    "\n",
    "# calculate average duration per timeframe\n",
    "duration_between_events = complete.groupby('time_aggregation').aggregate({'duration':'mean'})\n",
    "\n",
    "duration_between_events.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e283e6-cc77-412d-91ec-9915fcbc7764",
   "metadata": {},
   "source": [
    "## Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274e78b-c981-49f4-b30d-1a5729dc6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all active moments for each user\n",
    "user_moments = timeframe_df.groupby(['user_id', 'time_aggregation']).aggregate({'moment':'count'})\n",
    "\n",
    "# merge with first seen df\n",
    "user_activity = user_moments.merge(user_first_seen, how='inner', on='user_id')\n",
    "\n",
    "# clean-up and rename columns\n",
    "user_activity.rename(columns={'time_aggregation_min':'new_user_cohort'}, inplace=True)\n",
    "user_activity.drop(columns=['moment_count'], inplace=True)\n",
    "\n",
    "# for each new_user_cohort count how many users get back per timeframe\n",
    "retention_input = user_activity.groupby(['new_user_cohort', 'time_aggregation']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add the size of each new user cohort\n",
    "cohorts = retention_input.merge(new_users, how='inner', left_on='new_user_cohort', right_on='time_aggregation_min', suffixes=('_active', '_cohort'))\n",
    "\n",
    "# calculate classic retention (so not rolling retention, where users are required to be active each timeframe)\n",
    "cohorts['retention'] = cohorts['user_id_nunique_active'] / cohorts['user_id_nunique_cohort']\n",
    "\n",
    "# NOTE: once we can reset index. we should keep this in buh_tuh, so we can also view SQL etc.\n",
    "# now switch to Pandas, as the dataset is small enough\n",
    "cohorts_df = cohorts.to_df().reset_index()\n",
    "\n",
    "# create typical retention matrix\n",
    "cohorts_df = cohorts_df.astype({'new_user_cohort': 'int', 'time_aggregation': 'int'})\n",
    "cohorts_df['active_in_timeframe'] = cohorts_df.time_aggregation - cohorts_df.new_user_cohort\n",
    "cohorts_df.pivot('new_user_cohort', 'active_in_timeframe', 'retention')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdb781-d188-4f7c-b85d-d96262ff7e69",
   "metadata": {},
   "source": [
    "## Bounce rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81a91-e85c-4f96-b534-dd39fdacf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather sessions, hits per timeframe\n",
    "hits_sessions = timeframe_df[['time_aggregation', 'session_id', 'session_hit_number']]\n",
    "\n",
    "# calculate hits per session\n",
    "hits_per_session = hits_sessions.groupby(['time_aggregation', 'session_id']).aggregate({'session_hit_number':'nunique'})\n",
    "\n",
    "# select sessions with only one hit\n",
    "hit_selector = (hits_per_session['session_hit_number_nunique'] == 1)\n",
    "single_hit_sessions = hits_per_session[hit_selector].to_frame()\n",
    "\n",
    "# count these single hit sessions per timeframe\n",
    "bounced_sessions = single_hit_sessions.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "# merge with total sessions\n",
    "bounce_rate = bounced_sessions.merge(sessions, how='inner', on='time_aggregation', suffixes=('_bounce', '_total'))\n",
    "\n",
    "# calculate bounce rate\n",
    "bounce_rate['bounce_rate'] = bounce_rate['session_id_nunique_bounce'] / bounce_rate['session_id_nunique_total']\n",
    "\n",
    "# clean-up columns\n",
    "bounce_rate.drop(columns=['session_id_nunique_bounce', 'session_id_nunique_total'], inplace=True)\n",
    "\n",
    "bounce_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974447a7-774a-4713-bb22-61c6b41cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize bounce rate\n",
    "bounce_rate[['bounce_rate']].sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93759009-0366-446a-b759-3f6d5a9a8101",
   "metadata": {},
   "source": [
    "## User agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27cc99-43d2-42cc-8c7e-6273fa27e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df with the user_agent from the global contexts\n",
    "timeframe_df['user_agent'] = timeframe_df.global_contexts.json.user_agent\n",
    "\n",
    "# gather overall basic stats grouped per user_agent\n",
    "user_agent_counts = timeframe_df.groupby(['time_aggregation', 'user_agent']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per user_agent\n",
    "user_agent_counts['total_users'] = total_users[1]\n",
    "\n",
    "# calculate share per user_agent\n",
    "user_agent_counts['share_of_users'] = user_agent_counts['user_id_nunique'] / user_agent_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "user_agent_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "user_agent_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea590d1-973d-4a3a-a669-eb1e0781f2e6",
   "metadata": {},
   "source": [
    "## Referer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256be167-341e-41dc-ba99-9b42abf22388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to dataframe with the referer from the global contexts\n",
    "timeframe_df['referer'] = timeframe_df.global_contexts.json.get_from_context_with_type_series(type='HttpContext', key='referer')\n",
    "\n",
    "# gather overall basic stats grouped per referer\n",
    "referer_counts = timeframe_df.groupby(['time_aggregation', 'referer']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per referer\n",
    "referer_counts['total_users'] = total_users[1]\n",
    "\n",
    "# calculate share per referer\n",
    "referer_counts['share_of_users'] = referer_counts['user_id_nunique'] / referer_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "referer_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "referer_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd5068-0a4e-4bfb-9e4a-7ba1cfda81bc",
   "metadata": {},
   "source": [
    "## User timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928f24-9f52-4f44-871b-69fa9628b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the timeline of an indivual user's events\n",
    "# NOTE: we can make this better with feature selection & aggregation\n",
    "\n",
    "# select the spefic user we want to replay\n",
    "# NOTE: .astype('string') is more something buhtuh should handle, on list\n",
    "user_selector = (timeframe_df['user_id'].astype('string') == '320db8ee-847c-424b-8291-c65d021575aa')\n",
    "\n",
    "# create df with only this user's events\n",
    "selected_user_df = timeframe_df[user_selector]\n",
    "\n",
    "# NOTE: we can apply feature selection and maybe sankey visual here\n",
    "# timeline of this user's events\n",
    "user_timeline = selected_user_df[['moment','event_type', 'event_location', 'user_agent', 'referer']]\n",
    "\n",
    "user_timeline.sort_values(by='moment', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59aba6a-6bdb-4a2c-831e-a3655d597a88",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e3f77-417b-4e77-8157-81bdc93fc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total sessions per user\n",
    "total_sessions_user = timeframe_df.groupby(['user_id']).aggregate({'session_id':'nunique'})\n",
    "\n",
    "# calculate frequency\n",
    "frequency = total_sessions_user.groupby(['session_id_nunique']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per number of sessions\n",
    "frequency['share_of_users'] = frequency['user_id_nunique'] / total_users[1]\n",
    "\n",
    "frequency.sort_values(by='session_id_nunique', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f24409-afd6-4b80-ba15-b787cb219fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency\n",
    "frequency[['share_of_users']].sort_values(by='session_id_nunique', ascending=True).head(60).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3f8b0-8631-4f27-b09d-253dffd3ecd8",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dd9c9-e174-465c-8bf8-02515e092038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of active days per user\n",
    "user_active_check = timeframe_df.groupby(['user_id']).aggregate({'day':'nunique'})\n",
    "\n",
    "# select all users that had more than one active day\n",
    "user_active_check = user_active_check[(user_active_check['day_nunique'] > 1)]\n",
    "\n",
    "# select all active days for each user\n",
    "user_days = timeframe_df.groupby(['user_id', 'day']).aggregate({'time_aggregation':'min'})\n",
    "\n",
    "# merge with users that have more than one active day\n",
    "user_days = user_days.merge(user_active_check, how='inner', on='user_id')\n",
    "\n",
    "# reset the index so we can use the user_id & day columns\n",
    "user_days = user_days.reset_index()\n",
    "\n",
    "# get previous (because of the sorting) day for each user\n",
    "window = user_days.sort_values('day').window(['user_id'])\n",
    "user_days['prev_day'] = user_days.day.window_lag(window)\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "user_days = user_days.get_df_materialized_model()\n",
    "\n",
    "# calculate the number of days between an active day and prev_day\n",
    "user_days['recency'] = user_days['day'] - user_days['prev_day']\n",
    "\n",
    "# rename columns\n",
    "user_days.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate the recency per time_aggregation\n",
    "recency = user_days.groupby(['time_aggregation']).aggregate({'recency':'mean','user_id':'nunique'})\n",
    "\n",
    "recency.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdbaf6-6c0d-4f8c-874e-2ab7f8c96ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize recency\n",
    "recency[['recency_mean']].sort_values(by='time_aggregation', ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b98525-b831-4747-8085-4b883ab76f7e",
   "metadata": {},
   "source": [
    "## Get metrics to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cba1b-4018-4bb3-a014-bb748ac9728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're working on export functionality to dbt, until then, you can use view_sql() to get the SQL that runs on the full dataset for any metric above\n",
    "\n",
    "# As an example, the SQL for the session duration metric\n",
    "print(avg_session_duration.view_sql())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
