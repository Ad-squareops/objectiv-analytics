{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814152bb-a253-4003-a954-37d7e2677510",
   "metadata": {},
   "source": [
    "# Objectiv example notebook\n",
    "\n",
    "This notebook is a lightweight example to show what Objectiv can do for data collection & modeling. There is also a [web version ](https://notebook.objectiv.io/lab?path=product_analytics.ipynb) that contains real data from our website as an example.\n",
    "\n",
    "Once you have instrumented your website or app with the Objectiv tracker and have a PostgreSQL storage running, you can use this notebook to connect to that. The metrics are just an initial example of what Objectiv can do without the need for additional transformation or cleaning of data beforehand. Have a look at our [docs](https://docs.objectiv.io) for the full rundown.\n",
    "\n",
    "For any question, please join our [Slack channel](https://join.slack.com/t/objectiv-io/shared_invite/zt-u6xma89w-DLDvOB7pQer5QUs5B_~5pg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import sqlalchemy\n",
    "\n",
    "from jupyter_dash import JupyterDash as Dash\n",
    "from sankey_dash import get_app\n",
    "\n",
    "# import Objectiv Bach\n",
    "from bach import DataFrame\n",
    "sys.path.extend([\n",
    "    '../../bach',\n",
    "    '../'\n",
    "])\n",
    "\n",
    "from objectiv_bach.util import duplo_basic_features\n",
    "\n",
    "from bach.series.series_objectiv import FeatureFrame\n",
    "from bach.series.series_objectiv import SeriesGlobalContexts, SeriesLocationStack\n",
    "from bach.types import _registry\n",
    "_registry.register_dtype_series(SeriesGlobalContexts, [], override_registered_types=True)\n",
    "_registry.register_dtype_series(SeriesLocationStack, [], override_registered_types=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b47e4-e1c6-443f-a164-31078ed3ae8b",
   "metadata": {},
   "source": [
    "## Connect to full dataset in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to full postgresql dataset, add database and credentials here\n",
    "engine = sqlalchemy.create_engine('postgresql://objectiv:@localhost:5432/objectiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Bach dataframe based on the full dataset\n",
    "basic_features = duplo_basic_features()\n",
    "df = DataFrame.from_model(engine=engine, model=basic_features, index=['event_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab9da6-03b0-4dd1-8d69-6cabb5e930d3",
   "metadata": {},
   "source": [
    "## Set sample / unsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7a4cb-9453-4009-8ba3-cea3f86e9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if desired, sample the data to develop models, in this example case we keep it at 100%\n",
    "df = df.get_sample(table_name='basic_features_sample', sample_percentage=100, overwrite=True)\n",
    "\n",
    "# NOTE: if you ran this once, you need to put overwrite as True, otherwise error that it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b95780-2456-4fd6-adaf-280f686244e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unsample the data and run all models below on full dataset, use:\n",
    "# df = df.get_unsampled()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfc42c-b58f-4256-ab4a-8cd8ecfaa330",
   "metadata": {},
   "source": [
    "## Add global contexts & location stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b21e05-70d3-4e8c-ac0c-18dda5899d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the global contexts and location stack as custom dtype so we can use them in modeling\n",
    "df['global_contexts'] = df.global_contexts.astype('objectiv_global_context')\n",
    "df['location_stack'] = df.location_stack.astype('objectiv_location_stack')\n",
    "\n",
    "# add the event location from the location_stack as new column to the df, using ls function\n",
    "df['event_location'] = df.location_stack.ls.nice_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cd456-699f-4e92-b225-3b565504683d",
   "metadata": {},
   "source": [
    "## Set the user application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba425d-ccfd-4c4e-bf18-3a0195d605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df with the user application from the global contexts, using gc function\n",
    "df['user_application'] = df.global_contexts.gc.application\n",
    "\n",
    "# select one or more user application(s) for analysis, in this case objectiv.io website \n",
    "df = df[(df['user_application'] == 'objectiv-website')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e9883-842e-4c36-9e23-dc89be9ee9b8",
   "metadata": {},
   "source": [
    "## TEMP filter on IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05c9f8-2d75-4b7e-848f-2b2994501147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add IP as column using gc function and filter on it\n",
    "df['ip'] = df.global_contexts.gc.get_from_context_with_type_series(type='HttpContext', key='remote_address')\n",
    "\n",
    "df = df[(df['ip'] != '144.178.74.106') & \n",
    "        (df['ip'] != '77.160.42.36') &\n",
    "        (df['ip'] != '86.86.89.85')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4954-cabc-44e7-81b8-d18f9d303ed9",
   "metadata": {},
   "source": [
    "## Set the time aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a697920-a443-4593-8acd-71c514aa7406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose for which level of time aggregation the rest of the analysis will run\n",
    "# supports all Postgres datetime template patterns: https://www.postgresql.org/docs/9.1/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE\n",
    "\n",
    "agg_level = 'YYYYIW'\n",
    "\n",
    "# add the time aggregation as new column to the dataframes, so we can group on this later\n",
    "df['time_aggregation'] = df['moment'].format(agg_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a13bf-52e0-4082-a4a2-c382871b1713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set the timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177f1fb-c741-4269-8086-dbab942afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timeframe for analysis\n",
    "timeframe_selector = (df['moment'] >= datetime.date(2021,10,1)) & (df['moment'] < datetime.date(2021,11,8))\n",
    "\n",
    "# create a new df with timeframe applied \n",
    "timeframe_df = df[timeframe_selector]\n",
    "\n",
    "# explore the data\n",
    "timeframe_df.sort_values(by='moment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697e0d3-8443-427b-a3a9-09342adf624e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00ffd-9940-4ff3-b5db-2734730f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique users per timeframe\n",
    "users = timeframe_df.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# calculate total users, to reuse later\n",
    "total_users = timeframe_df['user_id'].nunique()\n",
    "\n",
    "users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d9b1e-5159-416d-a904-75b63b795e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize users\n",
    "fig = px.line(data_frame = users.head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f5454-f7c7-409b-a6d0-996e078c851a",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3c2a3-9616-43d6-b529-acf5f6d210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique sessions\n",
    "sessions = timeframe_df.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "sessions.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc334-17c9-4604-85dd-24fa43abaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sessions\n",
    "fig = px.line(data_frame = sessions.head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888435d-4287-4cdc-8767-985fe2b9c216",
   "metadata": {},
   "source": [
    "## Sessions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77706040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge users and sessions\n",
    "users_sessions = sessions.merge(users, how='inner', on='time_aggregation')\n",
    "\n",
    "# calculate average sessions per user\n",
    "users_sessions['sessions_per_user_avg'] = users_sessions['session_id_nunique'] / users_sessions['user_id_nunique']\n",
    "\n",
    "# clean-up columns\n",
    "users_sessions.drop(columns=['session_id_nunique', 'user_id_nunique'], inplace=True)\n",
    "\n",
    "users_sessions.sort_values('time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d52d8-08df-4133-a2da-6cedd81f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize average sessions per user\n",
    "fig = px.line(data_frame = users_sessions.head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaaf82-ee41-4602-a2cd-f622fa84b0b2",
   "metadata": {},
   "source": [
    "## New users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b084b1-06e8-401c-846e-eb2d749e4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first seen per user, based on dataset with not timeframe applied\n",
    "user_first_seen = df.groupby('user_id').aggregate({'time_aggregation':'min', 'session_id':'min'})\n",
    "\n",
    "# select all users that have been active in the time\n",
    "active_users = timeframe_df.groupby('user_id').aggregate({'session_id':'nunique'})\n",
    "\n",
    "# reset index so we can use user_id to join and clean-up columns\n",
    "active_users = active_users.reset_index()\n",
    "active_users.drop(columns=['session_id_nunique'], inplace=True)\n",
    "\n",
    "# merge with users that have been active in the timeframe\n",
    "user_first_seen = user_first_seen.merge(active_users, how='inner', on='user_id')\n",
    "\n",
    "# calculate new users for each timeframe\n",
    "new_users = user_first_seen.groupby('time_aggregation_min').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with total users to calculate ratio and limit to timerange\n",
    "new_total_users = users.merge(new_users, how='inner', left_on='time_aggregation', right_on='time_aggregation_min', suffixes=('_total', '_new'))\n",
    "\n",
    "# set time_aggregation as single index\n",
    "new_total_users = new_total_users.set_index('time_aggregation')\n",
    "\n",
    "# calculate new & returning user share\n",
    "new_total_users['new_user_share'] = new_total_users['user_id_nunique_new'] / new_total_users['user_id_nunique_total']\n",
    "new_total_users['returning_user_share'] = (new_total_users['user_id_nunique_total'] - new_total_users['user_id_nunique_new']) / new_total_users['user_id_nunique_total']\n",
    "\n",
    "new_total_users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d062-dd61-49c2-b669-bf0f45c68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize new users\n",
    "fig = px.line(data_frame = new_total_users[['user_id_nunique_new', 'user_id_nunique_total']].head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be740b11-d552-4501-b8b3-48cb077f8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize returning users\n",
    "fig = px.line(data_frame = new_total_users[['returning_user_share']].head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046eb7e6-fba5-4048-ba17-8cff59d92b38",
   "metadata": {},
   "source": [
    "## Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aeb0a-c5cf-4d69-9fe1-c25275026d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Objectiv, you can create features that utilize the context of where they occur on the UI, using the location stack\n",
    "\n",
    "# first, create a feature frame that will be used on create features\n",
    "feature_frame = FeatureFrame.from_data_frame(bt=timeframe_df, location_stack_column='location_stack', event_column='event_type')\n",
    "feature_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfe4ee-38a7-43a6-925e-a687fa73820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a Dash app, you can visualize all events with the location stack and create features\n",
    "\n",
    "# as an example, we'll create features:\n",
    "# 1) the job annoucement bar that is on both Home & About pages, and call these 'working_features'\n",
    "# 2) a started conversion funnel. called 'conversion_started'\n",
    "# 4) a completed conversion, called 'conversion_complete'\n",
    "\n",
    "app = get_app(Dash, feature_frame)\n",
    "app.run_server(mode='inline', height = 1000, port=8053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6dd8ee-9243-49e4-96a9-3d79873efc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can see that a new column is added for each feature we created, containing the location stack\n",
    "feature_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f870b-347e-41a3-bd5c-14e8f3933895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are happy with the result, write these creatured features to the working df\n",
    "timeframe_df = feature_frame.write_to_full_frame()\n",
    "timeframe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01e44e-e68e-4b84-93e6-b6a405a56a01",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c743b4-dae4-41b9-a4d5-15a87c3b295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features we just created\n",
    "created_features = timeframe_df[(timeframe_df.working_features.notnull()) | \n",
    "                                (timeframe_df.conversion_started.notnull()) |\n",
    "                                (timeframe_df.conversion_completed.notnull())]\n",
    "\n",
    "# get the number of total users and hits per feature\n",
    "users_per_event = created_features.groupby(['time_aggregation', 'event_type', 'event_location']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n",
    "\n",
    "users_per_event.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8695f-fbc9-4996-8b06-cea971d7baac",
   "metadata": {},
   "source": [
    "## New user features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dca3b5-4a48-454b-a519-bcb341b8a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 10 things new users do, for the features we created\n",
    "\n",
    "# get the first session for users that were new in the timeframe\n",
    "timeframe_new_users = created_features.merge(user_first_seen, how='inner', left_on=['user_id', 'time_aggregation', 'session_id'], right_on=['user_id', 'time_aggregation_min', 'session_id_min'])\n",
    "\n",
    "# limit to the first 10 events\n",
    "timeframe_new_users = timeframe_new_users[(timeframe_new_users['session_hit_number'] <= 10)]\n",
    "\n",
    "# number of total user and hits per feature\n",
    "new_user_events = timeframe_new_users.groupby(['time_aggregation', 'event_type', 'event_location']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n",
    "\n",
    "new_user_events.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf9cf5-c4e6-4d71-a15d-64bea1bf4d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e09155-217c-4631-b528-ccb7be25ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the created conversion feature\n",
    "conversion_completed = timeframe_df[(timeframe_df.conversion_completed.notnull()) &\n",
    "                                   (timeframe_df['event_type'] == 'CompletedEvent')]\n",
    "\n",
    "# calculate conversions, now per user, but can easily be aggregated to session_id instead\n",
    "conversions = conversion_completed.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with users, but can easily be done with sessions instead\n",
    "conversion_rate = conversions.merge(users, how='inner', on='time_aggregation', suffixes=('_converting', '_total'))\n",
    "\n",
    "# calculate conversion rate\n",
    "conversion_rate['conversion_rate'] = conversion_rate['user_id_nunique_converting'] / conversion_rate['user_id_nunique_total']\n",
    "\n",
    "conversion_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77c8ee-6038-4b9a-9403-09502899511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize conversion rate\n",
    "fig = px.line(data_frame = conversion_rate[['conversion_rate']].head())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a49676-1373-412b-b1bf-22974ad44d5b",
   "metadata": {},
   "source": [
    "## Conversion error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72e6e7-3b5d-4229-98a4-67fee7bf7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on the created conversion start events, this case a click on 'keep me posted' button\n",
    "conversion_start = timeframe_df[(timeframe_df.conversion_started.notnull()) &\n",
    "                                (timeframe_df['event_type'] == 'ClickEvent')]\n",
    "\n",
    "# calculate conversion starts, now per user, but can easily be aggregated to session_id instead\n",
    "conversion_starts = conversion_start.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# join conversion start & complete events\n",
    "conversion_totals = conversion_starts.merge(conversions, how='inner', on='time_aggregation')\n",
    "\n",
    "# rename columns\n",
    "conversion_totals.rename(columns={'user_id_nunique_x':'users_start','user_id_nunique_y':'users_completed'}, inplace=True)\n",
    "\n",
    "# calculate error rate by comparing starting and successfully completing a conversion event\n",
    "conversion_totals['error_rate'] = (conversion_totals['users_start'] - conversion_totals['users_completed']) / conversion_totals['users_start']\n",
    "\n",
    "conversion_totals.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84339df-e25a-43ff-a527-8ff7aa674128",
   "metadata": {},
   "source": [
    "## Conversion funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1af43-9714-4004-a104-aa7dd391b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for users that have a conversion event, select their conversion sessions and session_hit_number of the conversion moment\n",
    "converting_users = conversion_completed.groupby(['user_id', 'session_id']).aggregate({'session_hit_number':'max'})\n",
    "\n",
    "# merge with the df that has all user events in the timeframe\n",
    "converting_users_events = timeframe_df.merge(converting_users, how='inner', on=['user_id', 'session_id'])\n",
    "\n",
    "# select all events that converting users had up to their conversion moment in the same session\n",
    "converting_users_events = converting_users_events[(converting_users_events['session_hit_number'] <= converting_users_events['session_hit_number_max'])]\n",
    "\n",
    "# filter on only ClickEvent so we focus on user interactions\n",
    "converting_users_events = converting_users_events[(converting_users_events['event_type'] == 'ClickEvent')]\n",
    "\n",
    "# select all unique features used by these users\n",
    "converting_users_features = converting_users_events.groupby(['event_location']).aggregate({'event_id':'nunique'})\n",
    "\n",
    "# sort these features by name\n",
    "converting_users_features = converting_users_features.sort_values(by='event_location', ascending=True)\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "feature_id_pd = converting_users_features.to_pandas().reset_index()\n",
    "\n",
    "# clean-up columns\n",
    "feature_id_pd.drop(columns=['event_id_nunique'], inplace=True)\n",
    "\n",
    "# use the index to give each feature a unique id\n",
    "feature_id_pd['feature_id'] = feature_id_pd.index\n",
    "\n",
    "# create a window that returns the previous event for each row\n",
    "window = converting_users_events.sort_values('session_hit_number').window('session_id')\n",
    "converting_users_events['prev_event_location'] = converting_users_events.event_location.window_lag(window)\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "converting_users_events = converting_users_events.materialize()\n",
    "\n",
    "# group each unique event by previous unique event\n",
    "from_to_events = converting_users_events.groupby(['prev_event_location', 'event_location']).aggregate({'event_id':'nunique'})\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "from_to_events_pd = from_to_events.to_pandas().reset_index()\n",
    "\n",
    "# merge with the unique id for each prev_feature\n",
    "sankey_input_pd = from_to_events_pd.merge(feature_id_pd, how='inner', left_on='prev_event_location', right_on='event_location')\n",
    "sankey_input_pd = sankey_input_pd.rename(columns={'event_location_x':'event_location', 'feature_id':'prev_feature_id'})\n",
    "sankey_input_pd = sankey_input_pd.drop(columns={'event_location_y'})\n",
    "\n",
    "# merge with the unique id for each feature\n",
    "sankey_input_pd = sankey_input_pd.merge(feature_id_pd, how='left', left_on='event_location', right_on='event_location')\n",
    "\n",
    "# filter out events where prev_feature and feature are the same and user did not go anywhere new\n",
    "sankey_input_pd = sankey_input_pd[(sankey_input_pd['prev_feature_id'] != sankey_input_pd['feature_id'])]\n",
    "\n",
    "# sort by feature, so we can add the correct labels in the sankey\n",
    "#sankey_input = sankey_input.sort_values(by=['feature_id', 'prev_feature_id'], ascending=True)\n",
    "sankey_input_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852af5d8-e720-475b-af13-bc0d134a4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the sankey\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 50,\n",
    "      thickness = 5,\n",
    "      line = dict(color = \"black\", width = 1),\n",
    "# NOTE: @hendrik maybe we can come up with a better way to cut-off feature name?\n",
    "# have now added a hover with the full nice name\n",
    "      label = feature_id_pd['event_location'].str.slice(0,30).tolist(),\n",
    "      color = \"blue\",\n",
    "      customdata = feature_id_pd['event_location'].tolist(),\n",
    "      hovertemplate='%{customdata}<br />'+\n",
    "        'has value %{value}<extra></extra>'  \n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sankey_input_pd['prev_feature_id'].tolist(),\n",
    "      target = sankey_input_pd['feature_id'].tolist(),\n",
    "      value = sankey_input_pd['event_id_nunique'].tolist(),\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Conversion funnel\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c06f91-3fe1-4e25-8706-601e1b223393",
   "metadata": {},
   "source": [
    "## Session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279e793-e5a4-4a4b-968a-db8c5949f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate duration of each session\n",
    "session_duration = timeframe_df.groupby(['session_id']).aggregate({'moment':['min','max'], 'time_aggregation':'min'})\n",
    "session_duration['session_duration'] = session_duration['moment_max'] - session_duration['moment_min']\n",
    "\n",
    "# check which sessions have duration of zero and filter these out, as they are bounces\n",
    "session_duration = session_duration[(session_duration['session_duration'] > '0')]\n",
    "\n",
    "# rename columns\n",
    "session_duration.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate average session duration\n",
    "avg_session_duration = session_duration.groupby(['time_aggregation']).aggregate({'session_duration': 'mean'})\n",
    "\n",
    "avg_session_duration.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0f2c5-8e20-4b54-bdfa-dcb920794e5d",
   "metadata": {},
   "source": [
    "## Session duration for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ab57-27a1-4500-8335-293b94217684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the features we created, select oen or more to calculate duration for\n",
    "start_stop = timeframe_df[timeframe_df.working_features.notnull()]\n",
    "\n",
    "# get previous (because of the sorting) event for stop event _in the same session, window_lag(n) returns the nth previous value in the partition\n",
    "window = start_stop.sort_values('moment').window('session_id')\n",
    "start_stop['prev_event'] = start_stop.feature.window_lag(window)\n",
    "start_stop['prev_moment'] = start_stop.moment.window_lag(window)\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "start_stop = start_stop.materialize()\n",
    "\n",
    "# calculate duration\n",
    "start_stop['duration'] = start_stop.moment - start_stop.prev_moment\n",
    "\n",
    "# calculate average duration per timeframe\n",
    "duration_between_events = start_stop.groupby('time_aggregation').aggregate({'duration':'sum'})\n",
    "\n",
    "duration_between_events.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e283e6-cc77-412d-91ec-9915fcbc7764",
   "metadata": {},
   "source": [
    "## Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069417b1-e554-4e83-9dbb-692258a547eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all sorted time aggregations in the timeframe \n",
    "time_aggregations = timeframe_df.groupby(['time_aggregation']).aggregate({'user_id':'nunique'}).sort_values(by='time_aggregation', ascending=True)\n",
    "time_aggregations.head()\n",
    "\n",
    "# switch to Pandas as the dataset is small enough reset the index, use that to number each cohort\n",
    "time_cohorts = time_aggregations.to_pandas().reset_index()\n",
    "time_cohorts['cohort_id'] = time_cohorts.index\n",
    "time_cohorts.drop(columns=['user_id_nunique'], inplace=True)\n",
    "\n",
    "# select all active moments for each user\n",
    "user_moments = timeframe_df.groupby(['user_id', 'time_aggregation']).aggregate({'moment':'count'})\n",
    "\n",
    "# merge with first seen df\n",
    "user_activity = user_moments.merge(user_first_seen, how='inner', on='user_id')\n",
    "\n",
    "# clean-up and rename columns\n",
    "user_activity.rename(columns={'time_aggregation_min':'new_user_cohort'}, inplace=True)\n",
    "user_activity.drop(columns=['moment_count'], inplace=True)\n",
    "\n",
    "# limit new users to the selected timeframe\n",
    "timeframe_start = timeframe_df['time_aggregation'].min()\n",
    "user_activity = user_activity[(user_activity['new_user_cohort'] >= timeframe_start)]\n",
    "\n",
    "# for each new_user_cohort count how many users get back per timeframe\n",
    "retention_input = user_activity.groupby(['new_user_cohort', 'time_aggregation']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add the size of each new user cohort\n",
    "cohorts = retention_input.merge(new_users, how='inner', left_on='new_user_cohort', right_on='time_aggregation_min', suffixes=('_active', '_cohort'))\n",
    "\n",
    "# calculate classic retention (so not rolling retention, where users are required to be active each timeframe)\n",
    "cohorts['retention'] = cohorts['user_id_nunique_active'] / cohorts['user_id_nunique_cohort']\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "cohorts_pd = cohorts.to_pandas().reset_index()\n",
    "\n",
    "# merge with cohorts to lookup the id for each new user cohort\n",
    "cohorts_pd = cohorts_pd.merge(time_cohorts, how='inner', left_on='new_user_cohort', right_on='time_aggregation')\n",
    "cohorts_pd.drop(columns=['time_aggregation_y'], inplace=True)\n",
    "cohorts_pd.rename(columns={'cohort_id':'new_user_cohort_id', 'time_aggregation_x':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# merge with cohorts to lookup the id for each active user cohort\n",
    "cohorts_pd = cohorts_pd.merge(time_cohorts, how='inner', on='time_aggregation')\n",
    "cohorts_pd.rename(columns={'cohort_id':'active_user_cohort_id', 'time_aggregation_x':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# number the cohort in which users were active vs their new user cohort\n",
    "cohorts_pd['active_in_timeframe'] = cohorts_pd.active_user_cohort_id - cohorts_pd.new_user_cohort_id\n",
    "\n",
    "# create typical retention matrix\n",
    "cohorts_pd.pivot('new_user_cohort', 'active_in_timeframe', 'retention').replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b83230-19d7-48f9-8230-5f9540db8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove timeframe 0 where the new users are all there, for better visualisation\n",
    "cohorts_pd.drop(cohorts_pd[cohorts_pd.active_in_timeframe == 0].index, inplace=True)\n",
    "\n",
    "# create retention matrix\n",
    "retention_pd = cohorts_pd.pivot('new_user_cohort', 'active_in_timeframe', 'retention').replace(np.nan, 0)\n",
    "\n",
    "# visualize heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "fmt = lambda x,pos: '{:.0%}'.format(x)\n",
    "retention_heatmap = sns.heatmap(retention_pd, center=1, linewidths=1, square=True, annot=True, fmt=\".0%\", cbar_kws={'format': FuncFormatter(fmt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdb781-d188-4f7c-b85d-d96262ff7e69",
   "metadata": {},
   "source": [
    "## Bounce rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81a91-e85c-4f96-b534-dd39fdacf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather sessions, hits per timeframe\n",
    "hits_sessions = timeframe_df[['time_aggregation', 'session_id', 'session_hit_number']]\n",
    "\n",
    "# calculate hits per session\n",
    "hits_per_session = hits_sessions.groupby(['time_aggregation', 'session_id']).aggregate({'session_hit_number':'nunique'})\n",
    "\n",
    "# select sessions with only one hit\n",
    "hit_selector = (hits_per_session['session_hit_number_nunique'] == 1)\n",
    "single_hit_sessions = hits_per_session[hit_selector]\n",
    "\n",
    "# count these single hit sessions per timeframe\n",
    "bounced_sessions = single_hit_sessions.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "# merge with total sessions\n",
    "bounce_rate = bounced_sessions.merge(sessions, how='inner', on='time_aggregation', suffixes=('_bounce', '_total'))\n",
    "\n",
    "# calculate bounce rate\n",
    "bounce_rate['bounce_rate'] = bounce_rate['session_id_nunique_bounce'] / bounce_rate['session_id_nunique_total']\n",
    "\n",
    "# clean-up columns\n",
    "bounce_rate.drop(columns=['session_id_nunique_bounce', 'session_id_nunique_total'], inplace=True)\n",
    "\n",
    "bounce_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974447a7-774a-4713-bb22-61c6b41cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize bounce rate\n",
    "fig = px.line(data_frame = bounce_rate[['bounce_rate']].head(60))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93759009-0366-446a-b759-3f6d5a9a8101",
   "metadata": {},
   "source": [
    "## User agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27cc99-43d2-42cc-8c7e-6273fa27e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df with the user_agent from the global contexts, using gc function\n",
    "timeframe_df['user_agent'] = timeframe_df.global_contexts.gc.user_agent\n",
    "\n",
    "# gather overall basic stats grouped per user_agent\n",
    "user_agent_counts = timeframe_df.groupby(['time_aggregation', 'user_agent']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per user_agent\n",
    "user_agent_counts['total_users'] = total_users\n",
    "\n",
    "# calculate share per user_agent\n",
    "user_agent_counts['share_of_users'] = user_agent_counts['user_id_nunique'] / user_agent_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "user_agent_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "user_agent_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea590d1-973d-4a3a-a669-eb1e0781f2e6",
   "metadata": {},
   "source": [
    "## Referer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256be167-341e-41dc-ba99-9b42abf22388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to dataframe with the referer from the global contexts, using gc function\n",
    "timeframe_df['referer'] = timeframe_df.global_contexts.gc.get_from_context_with_type_series(type='HttpContext', key='referer')\n",
    "\n",
    "# gather overall basic stats grouped per referer\n",
    "referer_counts = timeframe_df.groupby(['time_aggregation', 'referer']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per referer\n",
    "referer_counts['total_users'] = total_users\n",
    "\n",
    "# calculate share per referer\n",
    "referer_counts['share_of_users'] = referer_counts['user_id_nunique'] / referer_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "referer_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "referer_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd5068-0a4e-4bfb-9e4a-7ba1cfda81bc",
   "metadata": {},
   "source": [
    "## User timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928f24-9f52-4f44-871b-69fa9628b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the spefic user we want to replay\n",
    "user_selector = (timeframe_df['user_id'].astype('string') == 'fe2657f1-a08c-4e33-b762-441c2f52855c')\n",
    "\n",
    "# create df with only this user's events\n",
    "selected_user_df = timeframe_df[user_selector]\n",
    "\n",
    "# timeline of this user's events\n",
    "user_timeline = selected_user_df[['moment','event_type', 'event_location', 'user_agent', 'referer']]\n",
    "\n",
    "user_timeline.sort_values(by='moment', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59aba6a-6bdb-4a2c-831e-a3655d597a88",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e3f77-417b-4e77-8157-81bdc93fc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total sessions per user\n",
    "total_sessions_user = timeframe_df.groupby(['user_id']).aggregate({'session_id':'nunique'})\n",
    "\n",
    "# calculate frequency\n",
    "frequency = total_sessions_user.groupby(['session_id_nunique']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per number of sessions\n",
    "frequency['share_of_users'] = frequency['user_id_nunique'] / total_users\n",
    "\n",
    "frequency.sort_values(by='session_id_nunique', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f24409-afd6-4b80-ba15-b787cb219fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency\n",
    "fig = px.bar(data_frame = frequency[['share_of_users']].head())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3f8b0-8631-4f27-b09d-253dffd3ecd8",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dd9c9-e174-465c-8bf8-02515e092038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of active days per user\n",
    "user_active_check = timeframe_df.groupby(['user_id']).aggregate({'day':'nunique'})\n",
    "\n",
    "# select all users that had more than one active day\n",
    "user_active_check = user_active_check[(user_active_check['day_nunique'] > 1)]\n",
    "\n",
    "# select all active days for each user\n",
    "user_days = timeframe_df.groupby(['user_id', 'day']).aggregate({'time_aggregation':'min'})\n",
    "\n",
    "# merge with users that have more than one active day\n",
    "user_days = user_days.merge(user_active_check, how='inner', on='user_id')\n",
    "\n",
    "# reset the index so we can use the user_id & day columns\n",
    "user_days = user_days.reset_index()\n",
    "\n",
    "# get previous (because of the sorting) day for each user\n",
    "window = user_days.sort_values('day').window(['user_id'])\n",
    "user_days['prev_day'] = user_days.day.window_lag(window)\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "user_days = user_days.materialize()\n",
    "\n",
    "# calculate the number of days between an active day and prev_day\n",
    "user_days['recency'] = user_days['day'] - user_days['prev_day']\n",
    "\n",
    "# rename columns\n",
    "user_days.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate the recency per time_aggregation\n",
    "recency = user_days.groupby(['time_aggregation']).aggregate({'recency':'mean','user_id':'nunique'})\n",
    "\n",
    "recency.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b98525-b831-4747-8085-4b883ab76f7e",
   "metadata": {},
   "source": [
    "## Get metrics to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cba1b-4018-4bb3-a014-bb748ac9728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're working on export functionality to dbt, until then, you can use view_sql() to get the SQL that runs on the full dataset for any metric above\n",
    "\n",
    "# as an example, the SQL for the session duration metric\n",
    "print(avg_session_duration.view_sql())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
